{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.rand(1000)\n",
    "y = 2*x1 + 5 + 0.5*np.random.rand(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x1, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x1.reshape((-1,1))\n",
    "y_train = y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(shape):\n",
    "    w = np.random.rand(shape[1],1)\n",
    "    w[-1] = 0\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(x,y,w, theta,alpha):\n",
    "    dw = x.T.dot(x.dot(w) - y)/x.shape[0] + alpha*np.sum(w)\n",
    "    w = w - theta*dw\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y,w):\n",
    "    return np.linalg.norm(y - x.dot(w), 2)/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x,y,theta = 0.0001,epochs = 1000, alpha = 0.01):\n",
    "    ones = np.ones((x.shape[0],1))\n",
    "    x = np.concatenate((ones, x), axis = 1)\n",
    "    w = weight_init(x.shape)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Training epoch: \" + str(epoch+1))\n",
    "        print(\"Loss: \" +str(loss(x,y,w)))\n",
    "        w = update_weight(x,y,w,theta,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Loss: 0.17833973121896424\n",
      "Training epoch: 2\n",
      "Loss: 0.17831722842782832\n",
      "Training epoch: 3\n",
      "Loss: 0.17829472848186037\n",
      "Training epoch: 4\n",
      "Loss: 0.17827223138070133\n",
      "Training epoch: 5\n",
      "Loss: 0.1782497371239922\n",
      "Training epoch: 6\n",
      "Loss: 0.17822724571137405\n",
      "Training epoch: 7\n",
      "Loss: 0.178204757142488\n",
      "Training epoch: 8\n",
      "Loss: 0.17818227141697518\n",
      "Training epoch: 9\n",
      "Loss: 0.17815978853447675\n",
      "Training epoch: 10\n",
      "Loss: 0.17813730849463402\n",
      "Training epoch: 11\n",
      "Loss: 0.17811483129708824\n",
      "Training epoch: 12\n",
      "Loss: 0.17809235694148082\n",
      "Training epoch: 13\n",
      "Loss: 0.178069885427453\n",
      "Training epoch: 14\n",
      "Loss: 0.17804741675464633\n",
      "Training epoch: 15\n",
      "Loss: 0.17802495092270215\n",
      "Training epoch: 16\n",
      "Loss: 0.17800248793126203\n",
      "Training epoch: 17\n",
      "Loss: 0.1779800277799676\n",
      "Training epoch: 18\n",
      "Loss: 0.17795757046846036\n",
      "Training epoch: 19\n",
      "Loss: 0.1779351159963821\n",
      "Training epoch: 20\n",
      "Loss: 0.1779126643633743\n",
      "Training epoch: 21\n",
      "Loss: 0.17789021556907886\n",
      "Training epoch: 22\n",
      "Loss: 0.17786776961313752\n",
      "Training epoch: 23\n",
      "Loss: 0.17784532649519208\n",
      "Training epoch: 24\n",
      "Loss: 0.1778228862148845\n",
      "Training epoch: 25\n",
      "Loss: 0.17780044877185666\n",
      "Training epoch: 26\n",
      "Loss: 0.17777801416575045\n",
      "Training epoch: 27\n",
      "Loss: 0.17775558239620798\n",
      "Training epoch: 28\n",
      "Loss: 0.17773315346287127\n",
      "Training epoch: 29\n",
      "Loss: 0.17771072736538243\n",
      "Training epoch: 30\n",
      "Loss: 0.1776883041033836\n",
      "Training epoch: 31\n",
      "Loss: 0.17766588367651695\n",
      "Training epoch: 32\n",
      "Loss: 0.1776434660844248\n",
      "Training epoch: 33\n",
      "Loss: 0.17762105132674932\n",
      "Training epoch: 34\n",
      "Loss: 0.17759863940313292\n",
      "Training epoch: 35\n",
      "Loss: 0.17757623031321793\n",
      "Training epoch: 36\n",
      "Loss: 0.17755382405664677\n",
      "Training epoch: 37\n",
      "Loss: 0.17753142063306193\n",
      "Training epoch: 38\n",
      "Loss: 0.17750902004210595\n",
      "Training epoch: 39\n",
      "Loss: 0.17748662228342127\n",
      "Training epoch: 40\n",
      "Loss: 0.17746422735665063\n",
      "Training epoch: 41\n",
      "Loss: 0.17744183526143656\n",
      "Training epoch: 42\n",
      "Loss: 0.1774194459974218\n",
      "Training epoch: 43\n",
      "Loss: 0.17739705956424912\n",
      "Training epoch: 44\n",
      "Loss: 0.17737467596156117\n",
      "Training epoch: 45\n",
      "Loss: 0.1773522951890009\n",
      "Training epoch: 46\n",
      "Loss: 0.17732991724621122\n",
      "Training epoch: 47\n",
      "Loss: 0.17730754213283484\n",
      "Training epoch: 48\n",
      "Loss: 0.17728516984851495\n",
      "Training epoch: 49\n",
      "Loss: 0.1772628003928944\n",
      "Training epoch: 50\n",
      "Loss: 0.17724043376561635\n",
      "Training epoch: 51\n",
      "Loss: 0.1772180699663238\n",
      "Training epoch: 52\n",
      "Loss: 0.17719570899465995\n",
      "Training epoch: 53\n",
      "Loss: 0.17717335085026797\n",
      "Training epoch: 54\n",
      "Loss: 0.17715099553279112\n",
      "Training epoch: 55\n",
      "Loss: 0.17712864304187262\n",
      "Training epoch: 56\n",
      "Loss: 0.1771062933771558\n",
      "Training epoch: 57\n",
      "Loss: 0.17708394653828405\n",
      "Training epoch: 58\n",
      "Loss: 0.1770616025249008\n",
      "Training epoch: 59\n",
      "Loss: 0.17703926133664946\n",
      "Training epoch: 60\n",
      "Loss: 0.17701692297317356\n",
      "Training epoch: 61\n",
      "Loss: 0.17699458743411664\n",
      "Training epoch: 62\n",
      "Loss: 0.17697225471912237\n",
      "Training epoch: 63\n",
      "Loss: 0.17694992482783423\n",
      "Training epoch: 64\n",
      "Loss: 0.17692759775989603\n",
      "Training epoch: 65\n",
      "Loss: 0.1769052735149514\n",
      "Training epoch: 66\n",
      "Loss: 0.1768829520926442\n",
      "Training epoch: 67\n",
      "Loss: 0.1768606334926182\n",
      "Training epoch: 68\n",
      "Loss: 0.17683831771451727\n",
      "Training epoch: 69\n",
      "Loss: 0.1768160047579853\n",
      "Training epoch: 70\n",
      "Loss: 0.1767936946226663\n",
      "Training epoch: 71\n",
      "Loss: 0.1767713873082043\n",
      "Training epoch: 72\n",
      "Loss: 0.17674908281424317\n",
      "Training epoch: 73\n",
      "Loss: 0.17672678114042717\n",
      "Training epoch: 74\n",
      "Loss: 0.1767044822864003\n",
      "Training epoch: 75\n",
      "Loss: 0.17668218625180682\n",
      "Training epoch: 76\n",
      "Loss: 0.1766598930362909\n",
      "Training epoch: 77\n",
      "Loss: 0.17663760263949685\n",
      "Training epoch: 78\n",
      "Loss: 0.17661531506106903\n",
      "Training epoch: 79\n",
      "Loss: 0.17659303030065168\n",
      "Training epoch: 80\n",
      "Loss: 0.1765707483578893\n",
      "Training epoch: 81\n",
      "Loss: 0.17654846923242626\n",
      "Training epoch: 82\n",
      "Loss: 0.17652619292390714\n",
      "Training epoch: 83\n",
      "Loss: 0.17650391943197644\n",
      "Training epoch: 84\n",
      "Loss: 0.17648164875627864\n",
      "Training epoch: 85\n",
      "Loss: 0.17645938089645852\n",
      "Training epoch: 86\n",
      "Loss: 0.17643711585216068\n",
      "Training epoch: 87\n",
      "Loss: 0.17641485362302983\n",
      "Training epoch: 88\n",
      "Loss: 0.1763925942087108\n",
      "Training epoch: 89\n",
      "Loss: 0.1763703376088483\n",
      "Training epoch: 90\n",
      "Loss: 0.17634808382308725\n",
      "Training epoch: 91\n",
      "Loss: 0.17632583285107248\n",
      "Training epoch: 92\n",
      "Loss: 0.17630358469244903\n",
      "Training epoch: 93\n",
      "Loss: 0.17628133934686174\n",
      "Training epoch: 94\n",
      "Loss: 0.1762590968139558\n",
      "Training epoch: 95\n",
      "Loss: 0.1762368570933762\n",
      "Training epoch: 96\n",
      "Loss: 0.1762146201847681\n",
      "Training epoch: 97\n",
      "Loss: 0.1761923860877766\n",
      "Training epoch: 98\n",
      "Loss: 0.17617015480204704\n",
      "Training epoch: 99\n",
      "Loss: 0.17614792632722448\n",
      "Training epoch: 100\n",
      "Loss: 0.17612570066295444\n",
      "Training epoch: 101\n",
      "Loss: 0.17610347780888205\n",
      "Training epoch: 102\n",
      "Loss: 0.17608125776465283\n",
      "Training epoch: 103\n",
      "Loss: 0.17605904052991225\n",
      "Training epoch: 104\n",
      "Loss: 0.17603682610430565\n",
      "Training epoch: 105\n",
      "Loss: 0.17601461448747865\n",
      "Training epoch: 106\n",
      "Loss: 0.17599240567907684\n",
      "Training epoch: 107\n",
      "Loss: 0.17597019967874578\n",
      "Training epoch: 108\n",
      "Loss: 0.1759479964861312\n",
      "Training epoch: 109\n",
      "Loss: 0.17592579610087863\n",
      "Training epoch: 110\n",
      "Loss: 0.17590359852263404\n",
      "Training epoch: 111\n",
      "Loss: 0.17588140375104308\n",
      "Training epoch: 112\n",
      "Loss: 0.17585921178575165\n",
      "Training epoch: 113\n",
      "Loss: 0.1758370226264056\n",
      "Training epoch: 114\n",
      "Loss: 0.17581483627265088\n",
      "Training epoch: 115\n",
      "Loss: 0.17579265272413344\n",
      "Training epoch: 116\n",
      "Loss: 0.17577047198049928\n",
      "Training epoch: 117\n",
      "Loss: 0.17574829404139455\n",
      "Training epoch: 118\n",
      "Loss: 0.17572611890646525\n",
      "Training epoch: 119\n",
      "Loss: 0.1757039465753576\n",
      "Training epoch: 120\n",
      "Loss: 0.17568177704771779\n",
      "Training epoch: 121\n",
      "Loss: 0.17565961032319202\n",
      "Training epoch: 122\n",
      "Loss: 0.1756374464014266\n",
      "Training epoch: 123\n",
      "Loss: 0.17561528528206788\n",
      "Training epoch: 124\n",
      "Loss: 0.1755931269647622\n",
      "Training epoch: 125\n",
      "Loss: 0.17557097144915604\n",
      "Training epoch: 126\n",
      "Loss: 0.17554881873489575\n",
      "Training epoch: 127\n",
      "Loss: 0.17552666882162798\n",
      "Training epoch: 128\n",
      "Loss: 0.17550452170899916\n",
      "Training epoch: 129\n",
      "Loss: 0.175482377396656\n",
      "Training epoch: 130\n",
      "Loss: 0.17546023588424503\n",
      "Training epoch: 131\n",
      "Loss: 0.17543809717141304\n",
      "Training epoch: 132\n",
      "Loss: 0.17541596125780667\n",
      "Training epoch: 133\n",
      "Loss: 0.17539382814307278\n",
      "Training epoch: 134\n",
      "Loss: 0.17537169782685813\n",
      "Training epoch: 135\n",
      "Loss: 0.17534957030880963\n",
      "Training epoch: 136\n",
      "Loss: 0.17532744558857416\n",
      "Training epoch: 137\n",
      "Loss: 0.1753053236657987\n",
      "Training epoch: 138\n",
      "Loss: 0.17528320454013027\n",
      "Training epoch: 139\n",
      "Loss: 0.1752610882112158\n",
      "Training epoch: 140\n",
      "Loss: 0.17523897467870253\n",
      "Training epoch: 141\n",
      "Loss: 0.17521686394223748\n",
      "Training epoch: 142\n",
      "Loss: 0.17519475600146794\n",
      "Training epoch: 143\n",
      "Loss: 0.17517265085604103\n",
      "Training epoch: 144\n",
      "Loss: 0.17515054850560413\n",
      "Training epoch: 145\n",
      "Loss: 0.17512844894980434\n",
      "Training epoch: 146\n",
      "Loss: 0.17510635218828924\n",
      "Training epoch: 147\n",
      "Loss: 0.17508425822070614\n",
      "Training epoch: 148\n",
      "Loss: 0.17506216704670252\n",
      "Training epoch: 149\n",
      "Loss: 0.17504007866592586\n",
      "Training epoch: 150\n",
      "Loss: 0.17501799307802363\n",
      "Training epoch: 151\n",
      "Loss: 0.17499591028264347\n",
      "Training epoch: 152\n",
      "Loss: 0.17497383027943303\n",
      "Training epoch: 153\n",
      "Loss: 0.17495175306803992\n",
      "Training epoch: 154\n",
      "Loss: 0.17492967864811193\n",
      "Training epoch: 155\n",
      "Loss: 0.1749076070192967\n",
      "Training epoch: 156\n",
      "Loss: 0.17488553818124217\n",
      "Training epoch: 157\n",
      "Loss: 0.17486347213359604\n",
      "Training epoch: 158\n",
      "Loss: 0.17484140887600638\n",
      "Training epoch: 159\n",
      "Loss: 0.17481934840812094\n",
      "Training epoch: 160\n",
      "Loss: 0.1747972907295878\n",
      "Training epoch: 161\n",
      "Loss: 0.17477523584005505\n",
      "Training epoch: 162\n",
      "Loss: 0.17475318373917062\n",
      "Training epoch: 163\n",
      "Loss: 0.17473113442658275\n",
      "Training epoch: 164\n",
      "Loss: 0.17470908790193948\n",
      "Training epoch: 165\n",
      "Loss: 0.17468704416488906\n",
      "Training epoch: 166\n",
      "Loss: 0.17466500321507977\n",
      "Training epoch: 167\n",
      "Loss: 0.17464296505215993\n",
      "Training epoch: 168\n",
      "Loss: 0.17462092967577775\n",
      "Training epoch: 169\n",
      "Loss: 0.1745988970855817\n",
      "Training epoch: 170\n",
      "Loss: 0.1745768672812202\n",
      "Training epoch: 171\n",
      "Loss: 0.1745548402623418\n",
      "Training epoch: 172\n",
      "Loss: 0.17453281602859483\n",
      "Training epoch: 173\n",
      "Loss: 0.17451079457962798\n",
      "Training epoch: 174\n",
      "Loss: 0.17448877591508982\n",
      "Training epoch: 175\n",
      "Loss: 0.17446676003462902\n",
      "Training epoch: 176\n",
      "Loss: 0.17444474693789425\n",
      "Training epoch: 177\n",
      "Loss: 0.17442273662453422\n",
      "Training epoch: 178\n",
      "Loss: 0.1744007290941978\n",
      "Training epoch: 179\n",
      "Loss: 0.17437872434653373\n",
      "Training epoch: 180\n",
      "Loss: 0.17435672238119088\n",
      "Training epoch: 181\n",
      "Loss: 0.17433472319781823\n",
      "Training epoch: 182\n",
      "Loss: 0.17431272679606466\n",
      "Training epoch: 183\n",
      "Loss: 0.17429073317557928\n",
      "Training epoch: 184\n",
      "Loss: 0.174268742336011\n",
      "Training epoch: 185\n",
      "Loss: 0.17424675427700906\n",
      "Training epoch: 186\n",
      "Loss: 0.17422476899822242\n",
      "Training epoch: 187\n",
      "Loss: 0.1742027864993004\n",
      "Training epoch: 188\n",
      "Loss: 0.17418080677989223\n",
      "Training epoch: 189\n",
      "Loss: 0.1741588298396471\n",
      "Training epoch: 190\n",
      "Loss: 0.17413685567821435\n",
      "Training epoch: 191\n",
      "Loss: 0.17411488429524336\n",
      "Training epoch: 192\n",
      "Loss: 0.1740929156903835\n",
      "Training epoch: 193\n",
      "Loss: 0.17407094986328422\n",
      "Training epoch: 194\n",
      "Loss: 0.17404898681359507\n",
      "Training epoch: 195\n",
      "Loss: 0.1740270265409655\n",
      "Training epoch: 196\n",
      "Loss: 0.1740050690450452\n",
      "Training epoch: 197\n",
      "Loss: 0.17398311432548363\n",
      "Training epoch: 198\n",
      "Loss: 0.17396116238193066\n",
      "Training epoch: 199\n",
      "Loss: 0.1739392132140358\n",
      "Training epoch: 200\n",
      "Loss: 0.173917266821449\n",
      "Training epoch: 201\n",
      "Loss: 0.17389532320381987\n",
      "Training epoch: 202\n",
      "Loss: 0.17387338236079838\n",
      "Training epoch: 203\n",
      "Loss: 0.1738514442920344\n",
      "Training epoch: 204\n",
      "Loss: 0.17382950899717783\n",
      "Training epoch: 205\n",
      "Loss: 0.17380757647587872\n",
      "Training epoch: 206\n",
      "Loss: 0.17378564672778699\n",
      "Training epoch: 207\n",
      "Loss: 0.17376371975255275\n",
      "Training epoch: 208\n",
      "Loss: 0.1737417955498262\n",
      "Training epoch: 209\n",
      "Loss: 0.17371987411925735\n",
      "Training epoch: 210\n",
      "Loss: 0.1736979554604965\n",
      "Training epoch: 211\n",
      "Loss: 0.17367603957319383\n",
      "Training epoch: 212\n",
      "Loss: 0.17365412645699968\n",
      "Training epoch: 213\n",
      "Loss: 0.1736322161115643\n",
      "Training epoch: 214\n",
      "Loss: 0.17361030853653822\n",
      "Training epoch: 215\n",
      "Loss: 0.1735884037315717\n",
      "Training epoch: 216\n",
      "Loss: 0.1735665016963153\n",
      "Training epoch: 217\n",
      "Loss: 0.1735446024304195\n",
      "Training epoch: 218\n",
      "Loss: 0.17352270593353483\n",
      "Training epoch: 219\n",
      "Loss: 0.1735008122053119\n",
      "Training epoch: 220\n",
      "Loss: 0.1734789212454014\n",
      "Training epoch: 221\n",
      "Loss: 0.17345703305345395\n",
      "Training epoch: 222\n",
      "Loss: 0.17343514762912032\n",
      "Training epoch: 223\n",
      "Loss: 0.17341326497205123\n",
      "Training epoch: 224\n",
      "Loss: 0.17339138508189764\n",
      "Training epoch: 225\n",
      "Loss: 0.17336950795831021\n",
      "Training epoch: 226\n",
      "Loss: 0.17334763360094\n",
      "Training epoch: 227\n",
      "Loss: 0.1733257620094379\n",
      "Training epoch: 228\n",
      "Loss: 0.17330389318345488\n",
      "Training epoch: 229\n",
      "Loss: 0.173282027122642\n",
      "Training epoch: 230\n",
      "Loss: 0.17326016382665038\n",
      "Training epoch: 231\n",
      "Loss: 0.17323830329513112\n",
      "Training epoch: 232\n",
      "Loss: 0.1732164455277354\n",
      "Training epoch: 233\n",
      "Loss: 0.1731945905241144\n",
      "Training epoch: 234\n",
      "Loss: 0.17317273828391944\n",
      "Training epoch: 235\n",
      "Loss: 0.17315088880680182\n",
      "Training epoch: 236\n",
      "Loss: 0.17312904209241278\n",
      "Training epoch: 237\n",
      "Loss: 0.17310719814040382\n",
      "Training epoch: 238\n",
      "Loss: 0.17308535695042637\n",
      "Training epoch: 239\n",
      "Loss: 0.1730635185221318\n",
      "Training epoch: 240\n",
      "Loss: 0.1730416828551718\n",
      "Training epoch: 241\n",
      "Loss: 0.17301984994919783\n",
      "Training epoch: 242\n",
      "Loss: 0.17299801980386154\n",
      "Training epoch: 243\n",
      "Loss: 0.1729761924188145\n",
      "Training epoch: 244\n",
      "Loss: 0.17295436779370854\n",
      "Training epoch: 245\n",
      "Loss: 0.17293254592819537\n",
      "Training epoch: 246\n",
      "Loss: 0.1729107268219267\n",
      "Training epoch: 247\n",
      "Loss: 0.17288891047455446\n",
      "Training epoch: 248\n",
      "Loss: 0.1728670968857305\n",
      "Training epoch: 249\n",
      "Loss: 0.17284528605510663\n",
      "Training epoch: 250\n",
      "Loss: 0.172823477982335\n",
      "Training epoch: 251\n",
      "Loss: 0.1728016726670675\n",
      "Training epoch: 252\n",
      "Loss: 0.17277987010895618\n",
      "Training epoch: 253\n",
      "Loss: 0.17275807030765317\n",
      "Training epoch: 254\n",
      "Loss: 0.17273627326281063\n",
      "Training epoch: 255\n",
      "Loss: 0.1727144789740807\n",
      "Training epoch: 256\n",
      "Loss: 0.17269268744111566\n",
      "Training epoch: 257\n",
      "Loss: 0.1726708986635677\n",
      "Training epoch: 258\n",
      "Loss: 0.17264911264108918\n",
      "Training epoch: 259\n",
      "Loss: 0.1726273293733325\n",
      "Training epoch: 260\n",
      "Loss: 0.17260554885994997\n",
      "Training epoch: 261\n",
      "Loss: 0.1725837711005942\n",
      "Training epoch: 262\n",
      "Loss: 0.17256199609491749\n",
      "Training epoch: 263\n",
      "Loss: 0.17254022384257245\n",
      "Training epoch: 264\n",
      "Loss: 0.1725184543432117\n",
      "Training epoch: 265\n",
      "Loss: 0.17249668759648784\n",
      "Training epoch: 266\n",
      "Loss: 0.17247492360205355\n",
      "Training epoch: 267\n",
      "Loss: 0.17245316235956148\n",
      "Training epoch: 268\n",
      "Loss: 0.17243140386866446\n",
      "Training epoch: 269\n",
      "Loss: 0.17240964812901524\n",
      "Training epoch: 270\n",
      "Loss: 0.17238789514026667\n",
      "Training epoch: 271\n",
      "Loss: 0.1723661449020717\n",
      "Training epoch: 272\n",
      "Loss: 0.17234439741408317\n",
      "Training epoch: 273\n",
      "Loss: 0.1723226526759541\n",
      "Training epoch: 274\n",
      "Loss: 0.1723009106873375\n",
      "Training epoch: 275\n",
      "Loss: 0.17227917144788643\n",
      "Training epoch: 276\n",
      "Loss: 0.172257434957254\n",
      "Training epoch: 277\n",
      "Loss: 0.1722357012150934\n",
      "Training epoch: 278\n",
      "Loss: 0.1722139702210578\n",
      "Training epoch: 279\n",
      "Loss: 0.17219224197480035\n",
      "Training epoch: 280\n",
      "Loss: 0.17217051647597445\n",
      "Training epoch: 281\n",
      "Loss: 0.1721487937242334\n",
      "Training epoch: 282\n",
      "Loss: 0.17212707371923047\n",
      "Training epoch: 283\n",
      "Loss: 0.17210535646061922\n",
      "Training epoch: 284\n",
      "Loss: 0.17208364194805303\n",
      "Training epoch: 285\n",
      "Loss: 0.1720619301811854\n",
      "Training epoch: 286\n",
      "Loss: 0.1720402211596699\n",
      "Training epoch: 287\n",
      "Loss: 0.17201851488316008\n",
      "Training epoch: 288\n",
      "Loss: 0.17199681135130962\n",
      "Training epoch: 289\n",
      "Loss: 0.17197511056377215\n",
      "Training epoch: 290\n",
      "Loss: 0.1719534125202015\n",
      "Training epoch: 291\n",
      "Loss: 0.17193171722025125\n",
      "Training epoch: 292\n",
      "Loss: 0.1719100246635753\n",
      "Training epoch: 293\n",
      "Loss: 0.17188833484982757\n",
      "Training epoch: 294\n",
      "Loss: 0.17186664777866187\n",
      "Training epoch: 295\n",
      "Loss: 0.17184496344973216\n",
      "Training epoch: 296\n",
      "Loss: 0.17182328186269238\n",
      "Training epoch: 297\n",
      "Loss: 0.1718016030171966\n",
      "Training epoch: 298\n",
      "Loss: 0.17177992691289895\n",
      "Training epoch: 299\n",
      "Loss: 0.17175825354945345\n",
      "Training epoch: 300\n",
      "Loss: 0.1717365829265143\n",
      "Training epoch: 301\n",
      "Loss: 0.17171491504373568\n",
      "Training epoch: 302\n",
      "Loss: 0.17169324990077187\n",
      "Training epoch: 303\n",
      "Loss: 0.17167158749727707\n",
      "Training epoch: 304\n",
      "Loss: 0.17164992783290575\n",
      "Training epoch: 305\n",
      "Loss: 0.17162827090731217\n",
      "Training epoch: 306\n",
      "Loss: 0.17160661672015082\n",
      "Training epoch: 307\n",
      "Loss: 0.17158496527107608\n",
      "Training epoch: 308\n",
      "Loss: 0.17156331655974252\n",
      "Training epoch: 309\n",
      "Loss: 0.17154167058580475\n",
      "Training epoch: 310\n",
      "Loss: 0.17152002734891725\n",
      "Training epoch: 311\n",
      "Loss: 0.17149838684873467\n",
      "Training epoch: 312\n",
      "Loss: 0.1714767490849118\n",
      "Training epoch: 313\n",
      "Loss: 0.17145511405710326\n",
      "Training epoch: 314\n",
      "Loss: 0.1714334817649639\n",
      "Training epoch: 315\n",
      "Loss: 0.17141185220814842\n",
      "Training epoch: 316\n",
      "Loss: 0.17139022538631174\n",
      "Training epoch: 317\n",
      "Loss: 0.1713686012991088\n",
      "Training epoch: 318\n",
      "Loss: 0.1713469799461945\n",
      "Training epoch: 319\n",
      "Loss: 0.1713253613272238\n",
      "Training epoch: 320\n",
      "Loss: 0.1713037454418518\n",
      "Training epoch: 321\n",
      "Loss: 0.1712821322897335\n",
      "Training epoch: 322\n",
      "Loss: 0.1712605218705241\n",
      "Training epoch: 323\n",
      "Loss: 0.17123891418387863\n",
      "Training epoch: 324\n",
      "Loss: 0.17121730922945252\n",
      "Training epoch: 325\n",
      "Loss: 0.1711957070069008\n",
      "Training epoch: 326\n",
      "Loss: 0.17117410751587886\n",
      "Training epoch: 327\n",
      "Loss: 0.17115251075604204\n",
      "Training epoch: 328\n",
      "Loss: 0.1711309167270456\n",
      "Training epoch: 329\n",
      "Loss: 0.17110932542854518\n",
      "Training epoch: 330\n",
      "Loss: 0.1710877368601961\n",
      "Training epoch: 331\n",
      "Loss: 0.17106615102165387\n",
      "Training epoch: 332\n",
      "Loss: 0.1710445679125741\n",
      "Training epoch: 333\n",
      "Loss: 0.17102298753261239\n",
      "Training epoch: 334\n",
      "Loss: 0.17100140988142432\n",
      "Training epoch: 335\n",
      "Loss: 0.17097983495866562\n",
      "Training epoch: 336\n",
      "Loss: 0.17095826276399195\n",
      "Training epoch: 337\n",
      "Loss: 0.1709366932970592\n",
      "Training epoch: 338\n",
      "Loss: 0.1709151265575231\n",
      "Training epoch: 339\n",
      "Loss: 0.17089356254503957\n",
      "Training epoch: 340\n",
      "Loss: 0.17087200125926444\n",
      "Training epoch: 341\n",
      "Loss: 0.17085044269985372\n",
      "Training epoch: 342\n",
      "Loss: 0.1708288868664633\n",
      "Training epoch: 343\n",
      "Loss: 0.17080733375874937\n",
      "Training epoch: 344\n",
      "Loss: 0.17078578337636788\n",
      "Training epoch: 345\n",
      "Loss: 0.17076423571897495\n",
      "Training epoch: 346\n",
      "Loss: 0.17074269078622684\n",
      "Training epoch: 347\n",
      "Loss: 0.17072114857777965\n",
      "Training epoch: 348\n",
      "Loss: 0.17069960909328968\n",
      "Training epoch: 349\n",
      "Loss: 0.17067807233241325\n",
      "Training epoch: 350\n",
      "Loss: 0.17065653829480665\n",
      "Training epoch: 351\n",
      "Loss: 0.17063500698012632\n",
      "Training epoch: 352\n",
      "Loss: 0.17061347838802857\n",
      "Training epoch: 353\n",
      "Loss: 0.17059195251816997\n",
      "Training epoch: 354\n",
      "Loss: 0.17057042937020706\n",
      "Training epoch: 355\n",
      "Loss: 0.17054890894379626\n",
      "Training epoch: 356\n",
      "Loss: 0.17052739123859428\n",
      "Training epoch: 357\n",
      "Loss: 0.1705058762542577\n",
      "Training epoch: 358\n",
      "Loss: 0.17048436399044323\n",
      "Training epoch: 359\n",
      "Loss: 0.17046285444680756\n",
      "Training epoch: 360\n",
      "Loss: 0.17044134762300758\n",
      "Training epoch: 361\n",
      "Loss: 0.17041984351869996\n",
      "Training epoch: 362\n",
      "Loss: 0.17039834213354166\n",
      "Training epoch: 363\n",
      "Loss: 0.17037684346718954\n",
      "Training epoch: 364\n",
      "Loss: 0.17035534751930048\n",
      "Training epoch: 365\n",
      "Loss: 0.1703338542895316\n",
      "Training epoch: 366\n",
      "Loss: 0.17031236377753986\n",
      "Training epoch: 367\n",
      "Loss: 0.17029087598298231\n",
      "Training epoch: 368\n",
      "Loss: 0.17026939090551615\n",
      "Training epoch: 369\n",
      "Loss: 0.17024790854479843\n",
      "Training epoch: 370\n",
      "Loss: 0.1702264289004865\n",
      "Training epoch: 371\n",
      "Loss: 0.17020495197223745\n",
      "Training epoch: 372\n",
      "Loss: 0.17018347775970868\n",
      "Training epoch: 373\n",
      "Loss: 0.17016200626255745\n",
      "Training epoch: 374\n",
      "Loss: 0.1701405374804412\n",
      "Training epoch: 375\n",
      "Loss: 0.17011907141301735\n",
      "Training epoch: 376\n",
      "Loss: 0.17009760805994334\n",
      "Training epoch: 377\n",
      "Loss: 0.17007614742087668\n",
      "Training epoch: 378\n",
      "Loss: 0.17005468949547495\n",
      "Training epoch: 379\n",
      "Loss: 0.17003323428339567\n",
      "Training epoch: 380\n",
      "Loss: 0.1700117817842966\n",
      "Training epoch: 381\n",
      "Loss: 0.16999033199783525\n",
      "Training epoch: 382\n",
      "Loss: 0.16996888492366952\n",
      "Training epoch: 383\n",
      "Loss: 0.16994744056145708\n",
      "Training epoch: 384\n",
      "Loss: 0.16992599891085577\n",
      "Training epoch: 385\n",
      "Loss: 0.16990455997152343\n",
      "Training epoch: 386\n",
      "Loss: 0.169883123743118\n",
      "Training epoch: 387\n",
      "Loss: 0.16986169022529735\n",
      "Training epoch: 388\n",
      "Loss: 0.16984025941771952\n",
      "Training epoch: 389\n",
      "Loss: 0.16981883132004252\n",
      "Training epoch: 390\n",
      "Loss: 0.16979740593192438\n",
      "Training epoch: 391\n",
      "Loss: 0.16977598325302334\n",
      "Training epoch: 392\n",
      "Loss: 0.16975456328299743\n",
      "Training epoch: 393\n",
      "Loss: 0.16973314602150485\n",
      "Training epoch: 394\n",
      "Loss: 0.16971173146820392\n",
      "Training epoch: 395\n",
      "Loss: 0.1696903196227529\n",
      "Training epoch: 396\n",
      "Loss: 0.16966891048481014\n",
      "Training epoch: 397\n",
      "Loss: 0.16964750405403392\n",
      "Training epoch: 398\n",
      "Loss: 0.16962610033008277\n",
      "Training epoch: 399\n",
      "Loss: 0.16960469931261507\n",
      "Training epoch: 400\n",
      "Loss: 0.1695833010012894\n",
      "Training epoch: 401\n",
      "Loss: 0.16956190539576424\n",
      "Training epoch: 402\n",
      "Loss: 0.1695405124956982\n",
      "Training epoch: 403\n",
      "Loss: 0.16951912230074995\n",
      "Training epoch: 404\n",
      "Loss: 0.1694977348105781\n",
      "Training epoch: 405\n",
      "Loss: 0.16947635002484138\n",
      "Training epoch: 406\n",
      "Loss: 0.16945496794319856\n",
      "Training epoch: 407\n",
      "Loss: 0.1694335885653085\n",
      "Training epoch: 408\n",
      "Loss: 0.16941221189082997\n",
      "Training epoch: 409\n",
      "Loss: 0.1693908379194219\n",
      "Training epoch: 410\n",
      "Loss: 0.1693694666507432\n",
      "Training epoch: 411\n",
      "Loss: 0.16934809808445292\n",
      "Training epoch: 412\n",
      "Loss: 0.1693267322202099\n",
      "Training epoch: 413\n",
      "Loss: 0.16930536905767346\n",
      "Training epoch: 414\n",
      "Loss: 0.16928400859650256\n",
      "Training epoch: 415\n",
      "Loss: 0.16926265083635636\n",
      "Training epoch: 416\n",
      "Loss: 0.1692412957768941\n",
      "Training epoch: 417\n",
      "Loss: 0.16921994341777485\n",
      "Training epoch: 418\n",
      "Loss: 0.1691985937586581\n",
      "Training epoch: 419\n",
      "Loss: 0.16917724679920307\n",
      "Training epoch: 420\n",
      "Loss: 0.16915590253906918\n",
      "Training epoch: 421\n",
      "Loss: 0.16913456097791577\n",
      "Training epoch: 422\n",
      "Loss: 0.16911322211540236\n",
      "Training epoch: 423\n",
      "Loss: 0.16909188595118838\n",
      "Training epoch: 424\n",
      "Loss: 0.16907055248493338\n",
      "Training epoch: 425\n",
      "Loss: 0.169049221716297\n",
      "Training epoch: 426\n",
      "Loss: 0.16902789364493873\n",
      "Training epoch: 427\n",
      "Loss: 0.1690065682705184\n",
      "Training epoch: 428\n",
      "Loss: 0.16898524559269565\n",
      "Training epoch: 429\n",
      "Loss: 0.16896392561113016\n",
      "Training epoch: 430\n",
      "Loss: 0.16894260832548186\n",
      "Training epoch: 431\n",
      "Loss: 0.16892129373541057\n",
      "Training epoch: 432\n",
      "Loss: 0.16889998184057606\n",
      "Training epoch: 433\n",
      "Loss: 0.16887867264063833\n",
      "Training epoch: 434\n",
      "Loss: 0.16885736613525734\n",
      "Training epoch: 435\n",
      "Loss: 0.16883606232409312\n",
      "Training epoch: 436\n",
      "Loss: 0.16881476120680566\n",
      "Training epoch: 437\n",
      "Loss: 0.16879346278305513\n",
      "Training epoch: 438\n",
      "Loss: 0.16877216705250167\n",
      "Training epoch: 439\n",
      "Loss: 0.1687508740148054\n",
      "Training epoch: 440\n",
      "Loss: 0.16872958366962656\n",
      "Training epoch: 441\n",
      "Loss: 0.16870829601662546\n",
      "Training epoch: 442\n",
      "Loss: 0.1686870110554624\n",
      "Training epoch: 443\n",
      "Loss: 0.16866572878579775\n",
      "Training epoch: 444\n",
      "Loss: 0.16864444920729182\n",
      "Training epoch: 445\n",
      "Loss: 0.1686231723196051\n",
      "Training epoch: 446\n",
      "Loss: 0.16860189812239815\n",
      "Training epoch: 447\n",
      "Loss: 0.1685806266153314\n",
      "Training epoch: 448\n",
      "Loss: 0.1685593577980655\n",
      "Training epoch: 449\n",
      "Loss: 0.16853809167026093\n",
      "Training epoch: 450\n",
      "Loss: 0.16851682823157849\n",
      "Training epoch: 451\n",
      "Loss: 0.16849556748167877\n",
      "Training epoch: 452\n",
      "Loss: 0.16847430942022262\n",
      "Training epoch: 453\n",
      "Loss: 0.16845305404687066\n",
      "Training epoch: 454\n",
      "Loss: 0.16843180136128394\n",
      "Training epoch: 455\n",
      "Loss: 0.1684105513631231\n",
      "Training epoch: 456\n",
      "Loss: 0.16838930405204927\n",
      "Training epoch: 457\n",
      "Loss: 0.16836805942772323\n",
      "Training epoch: 458\n",
      "Loss: 0.16834681748980607\n",
      "Training epoch: 459\n",
      "Loss: 0.16832557823795877\n",
      "Training epoch: 460\n",
      "Loss: 0.16830434167184247\n",
      "Training epoch: 461\n",
      "Loss: 0.16828310779111827\n",
      "Training epoch: 462\n",
      "Loss: 0.16826187659544733\n",
      "Training epoch: 463\n",
      "Loss: 0.16824064808449093\n",
      "Training epoch: 464\n",
      "Loss: 0.16821942225791028\n",
      "Training epoch: 465\n",
      "Loss: 0.16819819911536663\n",
      "Training epoch: 466\n",
      "Loss: 0.16817697865652137\n",
      "Training epoch: 467\n",
      "Loss: 0.16815576088103584\n",
      "Training epoch: 468\n",
      "Loss: 0.1681345457885716\n",
      "Training epoch: 469\n",
      "Loss: 0.16811333337878992\n",
      "Training epoch: 470\n",
      "Loss: 0.16809212365135245\n",
      "Training epoch: 471\n",
      "Loss: 0.1680709166059207\n",
      "Training epoch: 472\n",
      "Loss: 0.16804971224215629\n",
      "Training epoch: 473\n",
      "Loss: 0.16802851055972087\n",
      "Training epoch: 474\n",
      "Loss: 0.16800731155827606\n",
      "Training epoch: 475\n",
      "Loss: 0.16798611523748364\n",
      "Training epoch: 476\n",
      "Loss: 0.16796492159700538\n",
      "Training epoch: 477\n",
      "Loss: 0.16794373063650306\n",
      "Training epoch: 478\n",
      "Loss: 0.16792254235563864\n",
      "Training epoch: 479\n",
      "Loss: 0.1679013567540738\n",
      "Training epoch: 480\n",
      "Loss: 0.16788017383147066\n",
      "Training epoch: 481\n",
      "Loss: 0.16785899358749118\n",
      "Training epoch: 482\n",
      "Loss: 0.16783781602179734\n",
      "Training epoch: 483\n",
      "Loss: 0.1678166411340513\n",
      "Training epoch: 484\n",
      "Loss: 0.16779546892391498\n",
      "Training epoch: 485\n",
      "Loss: 0.16777429939105065\n",
      "Training epoch: 486\n",
      "Loss: 0.16775313253512059\n",
      "Training epoch: 487\n",
      "Loss: 0.16773196835578694\n",
      "Training epoch: 488\n",
      "Loss: 0.16771080685271192\n",
      "Training epoch: 489\n",
      "Loss: 0.167689648025558\n",
      "Training epoch: 490\n",
      "Loss: 0.16766849187398744\n",
      "Training epoch: 491\n",
      "Loss: 0.16764733839766277\n",
      "Training epoch: 492\n",
      "Loss: 0.16762618759624626\n",
      "Training epoch: 493\n",
      "Loss: 0.16760503946940056\n",
      "Training epoch: 494\n",
      "Loss: 0.16758389401678817\n",
      "Training epoch: 495\n",
      "Loss: 0.16756275123807168\n",
      "Training epoch: 496\n",
      "Loss: 0.16754161113291363\n",
      "Training epoch: 497\n",
      "Loss: 0.16752047370097675\n",
      "Training epoch: 498\n",
      "Loss: 0.16749933894192376\n",
      "Training epoch: 499\n",
      "Loss: 0.1674782068554174\n",
      "Training epoch: 500\n",
      "Loss: 0.16745707744112046\n",
      "Training epoch: 501\n",
      "Loss: 0.16743595069869577\n",
      "Training epoch: 502\n",
      "Loss: 0.1674148266278062\n",
      "Training epoch: 503\n",
      "Loss: 0.1673937052281147\n",
      "Training epoch: 504\n",
      "Loss: 0.16737258649928421\n",
      "Training epoch: 505\n",
      "Loss: 0.16735147044097776\n",
      "Training epoch: 506\n",
      "Loss: 0.16733035705285837\n",
      "Training epoch: 507\n",
      "Loss: 0.16730924633458916\n",
      "Training epoch: 508\n",
      "Loss: 0.16728813828583328\n",
      "Training epoch: 509\n",
      "Loss: 0.16726703290625383\n",
      "Training epoch: 510\n",
      "Loss: 0.16724593019551412\n",
      "Training epoch: 511\n",
      "Loss: 0.16722483015327733\n",
      "Training epoch: 512\n",
      "Loss: 0.1672037327792069\n",
      "Training epoch: 513\n",
      "Loss: 0.16718263807296596\n",
      "Training epoch: 514\n",
      "Loss: 0.16716154603421815\n",
      "Training epoch: 515\n",
      "Loss: 0.16714045666262672\n",
      "Training epoch: 516\n",
      "Loss: 0.16711936995785517\n",
      "Training epoch: 517\n",
      "Loss: 0.16709828591956713\n",
      "Training epoch: 518\n",
      "Loss: 0.16707720454742606\n",
      "Training epoch: 519\n",
      "Loss: 0.1670561258410955\n",
      "Training epoch: 520\n",
      "Loss: 0.1670350498002393\n",
      "Training epoch: 521\n",
      "Loss: 0.16701397642452095\n",
      "Training epoch: 522\n",
      "Loss: 0.16699290571360428\n",
      "Training epoch: 523\n",
      "Loss: 0.16697183766715307\n",
      "Training epoch: 524\n",
      "Loss: 0.16695077228483113\n",
      "Training epoch: 525\n",
      "Loss: 0.1669297095663022\n",
      "Training epoch: 526\n",
      "Loss: 0.16690864951123038\n",
      "Training epoch: 527\n",
      "Loss: 0.16688759211927948\n",
      "Training epoch: 528\n",
      "Loss: 0.1668665373901135\n",
      "Training epoch: 529\n",
      "Loss: 0.1668454853233965\n",
      "Training epoch: 530\n",
      "Loss: 0.16682443591879248\n",
      "Training epoch: 531\n",
      "Loss: 0.16680338917596568\n",
      "Training epoch: 532\n",
      "Loss: 0.16678234509458018\n",
      "Training epoch: 533\n",
      "Loss: 0.1667613036743002\n",
      "Training epoch: 534\n",
      "Loss: 0.16674026491478988\n",
      "Training epoch: 535\n",
      "Loss: 0.16671922881571363\n",
      "Training epoch: 536\n",
      "Loss: 0.16669819537673575\n",
      "Training epoch: 537\n",
      "Loss: 0.16667716459752058\n",
      "Training epoch: 538\n",
      "Loss: 0.16665613647773253\n",
      "Training epoch: 539\n",
      "Loss: 0.16663511101703607\n",
      "Training epoch: 540\n",
      "Loss: 0.16661408821509568\n",
      "Training epoch: 541\n",
      "Loss: 0.16659306807157592\n",
      "Training epoch: 542\n",
      "Loss: 0.16657205058614133\n",
      "Training epoch: 543\n",
      "Loss: 0.16655103575845656\n",
      "Training epoch: 544\n",
      "Loss: 0.16653002358818628\n",
      "Training epoch: 545\n",
      "Loss: 0.16650901407499522\n",
      "Training epoch: 546\n",
      "Loss: 0.16648800721854803\n",
      "Training epoch: 547\n",
      "Loss: 0.1664670030185096\n",
      "Training epoch: 548\n",
      "Loss: 0.16644600147454475\n",
      "Training epoch: 549\n",
      "Loss: 0.16642500258631832\n",
      "Training epoch: 550\n",
      "Loss: 0.16640400635349528\n",
      "Training epoch: 551\n",
      "Loss: 0.16638301277574052\n",
      "Training epoch: 552\n",
      "Loss: 0.16636202185271912\n",
      "Training epoch: 553\n",
      "Loss: 0.1663410335840961\n",
      "Training epoch: 554\n",
      "Loss: 0.16632004796953648\n",
      "Training epoch: 555\n",
      "Loss: 0.16629906500870553\n",
      "Training epoch: 556\n",
      "Loss: 0.1662780847012683\n",
      "Training epoch: 557\n",
      "Loss: 0.16625710704689003\n",
      "Training epoch: 558\n",
      "Loss: 0.16623613204523605\n",
      "Training epoch: 559\n",
      "Loss: 0.16621515969597161\n",
      "Training epoch: 560\n",
      "Loss: 0.166194189998762\n",
      "Training epoch: 561\n",
      "Loss: 0.16617322295327266\n",
      "Training epoch: 562\n",
      "Loss: 0.166152258559169\n",
      "Training epoch: 563\n",
      "Loss: 0.16613129681611657\n",
      "Training epoch: 564\n",
      "Loss: 0.16611033772378078\n",
      "Training epoch: 565\n",
      "Loss: 0.1660893812818272\n",
      "Training epoch: 566\n",
      "Loss: 0.16606842748992148\n",
      "Training epoch: 567\n",
      "Loss: 0.16604747634772918\n",
      "Training epoch: 568\n",
      "Loss: 0.1660265278549161\n",
      "Training epoch: 569\n",
      "Loss: 0.1660055820111478\n",
      "Training epoch: 570\n",
      "Loss: 0.16598463881609013\n",
      "Training epoch: 571\n",
      "Loss: 0.165963698269409\n",
      "Training epoch: 572\n",
      "Loss: 0.16594276037077005\n",
      "Training epoch: 573\n",
      "Loss: 0.1659218251198393\n",
      "Training epoch: 574\n",
      "Loss: 0.1659008925162827\n",
      "Training epoch: 575\n",
      "Loss: 0.1658799625597662\n",
      "Training epoch: 576\n",
      "Loss: 0.1658590352499558\n",
      "Training epoch: 577\n",
      "Loss: 0.16583811058651762\n",
      "Training epoch: 578\n",
      "Loss: 0.1658171885691176\n",
      "Training epoch: 579\n",
      "Loss: 0.1657962691974221\n",
      "Training epoch: 580\n",
      "Loss: 0.16577535247109718\n",
      "Training epoch: 581\n",
      "Loss: 0.16575443838980908\n",
      "Training epoch: 582\n",
      "Loss: 0.1657335269532241\n",
      "Training epoch: 583\n",
      "Loss: 0.16571261816100857\n",
      "Training epoch: 584\n",
      "Loss: 0.16569171201282878\n",
      "Training epoch: 585\n",
      "Loss: 0.16567080850835125\n",
      "Training epoch: 586\n",
      "Loss: 0.16564990764724233\n",
      "Training epoch: 587\n",
      "Loss: 0.16562900942916847\n",
      "Training epoch: 588\n",
      "Loss: 0.16560811385379626\n",
      "Training epoch: 589\n",
      "Loss: 0.16558722092079223\n",
      "Training epoch: 590\n",
      "Loss: 0.165566330629823\n",
      "Training epoch: 591\n",
      "Loss: 0.16554544298055526\n",
      "Training epoch: 592\n",
      "Loss: 0.16552455797265567\n",
      "Training epoch: 593\n",
      "Loss: 0.16550367560579096\n",
      "Training epoch: 594\n",
      "Loss: 0.16548279587962797\n",
      "Training epoch: 595\n",
      "Loss: 0.1654619187938334\n",
      "Training epoch: 596\n",
      "Loss: 0.16544104434807422\n",
      "Training epoch: 597\n",
      "Loss: 0.1654201725420173\n",
      "Training epoch: 598\n",
      "Loss: 0.16539930337532957\n",
      "Training epoch: 599\n",
      "Loss: 0.16537843684767806\n",
      "Training epoch: 600\n",
      "Loss: 0.16535757295872971\n",
      "Training epoch: 601\n",
      "Loss: 0.1653367117081517\n",
      "Training epoch: 602\n",
      "Loss: 0.16531585309561111\n",
      "Training epoch: 603\n",
      "Loss: 0.16529499712077506\n",
      "Training epoch: 604\n",
      "Loss: 0.1652741437833108\n",
      "Training epoch: 605\n",
      "Loss: 0.16525329308288558\n",
      "Training epoch: 606\n",
      "Loss: 0.16523244501916654\n",
      "Training epoch: 607\n",
      "Loss: 0.16521159959182125\n",
      "Training epoch: 608\n",
      "Loss: 0.1651907568005169\n",
      "Training epoch: 609\n",
      "Loss: 0.1651699166449209\n",
      "Training epoch: 610\n",
      "Loss: 0.1651490791247008\n",
      "Training epoch: 611\n",
      "Loss: 0.16512824423952405\n",
      "Training epoch: 612\n",
      "Loss: 0.16510741198905812\n",
      "Training epoch: 613\n",
      "Loss: 0.1650865823729707\n",
      "Training epoch: 614\n",
      "Loss: 0.16506575539092933\n",
      "Training epoch: 615\n",
      "Loss: 0.16504493104260168\n",
      "Training epoch: 616\n",
      "Loss: 0.16502410932765552\n",
      "Training epoch: 617\n",
      "Loss: 0.1650032902457585\n",
      "Training epoch: 618\n",
      "Loss: 0.16498247379657846\n",
      "Training epoch: 619\n",
      "Loss: 0.16496165997978327\n",
      "Training epoch: 620\n",
      "Loss: 0.16494084879504073\n",
      "Training epoch: 621\n",
      "Loss: 0.16492004024201878\n",
      "Training epoch: 622\n",
      "Loss: 0.16489923432038534\n",
      "Training epoch: 623\n",
      "Loss: 0.16487843102980843\n",
      "Training epoch: 624\n",
      "Loss: 0.1648576303699562\n",
      "Training epoch: 625\n",
      "Loss: 0.1648368323404966\n",
      "Training epoch: 626\n",
      "Loss: 0.16481603694109778\n",
      "Training epoch: 627\n",
      "Loss: 0.16479524417142793\n",
      "Training epoch: 628\n",
      "Loss: 0.1647744540311552\n",
      "Training epoch: 629\n",
      "Loss: 0.16475366651994797\n",
      "Training epoch: 630\n",
      "Loss: 0.16473288163747435\n",
      "Training epoch: 631\n",
      "Loss: 0.16471209938340284\n",
      "Training epoch: 632\n",
      "Loss: 0.16469131975740173\n",
      "Training epoch: 633\n",
      "Loss: 0.1646705427591394\n",
      "Training epoch: 634\n",
      "Loss: 0.1646497683882844\n",
      "Training epoch: 635\n",
      "Loss: 0.1646289966445052\n",
      "Training epoch: 636\n",
      "Loss: 0.1646082275274703\n",
      "Training epoch: 637\n",
      "Loss: 0.1645874610368484\n",
      "Training epoch: 638\n",
      "Loss: 0.16456669717230799\n",
      "Training epoch: 639\n",
      "Loss: 0.1645459359335178\n",
      "Training epoch: 640\n",
      "Loss: 0.16452517732014654\n",
      "Training epoch: 641\n",
      "Loss: 0.164504421331863\n",
      "Training epoch: 642\n",
      "Loss: 0.16448366796833586\n",
      "Training epoch: 643\n",
      "Loss: 0.16446291722923415\n",
      "Training epoch: 644\n",
      "Loss: 0.16444216911422652\n",
      "Training epoch: 645\n",
      "Loss: 0.164421423622982\n",
      "Training epoch: 646\n",
      "Loss: 0.16440068075516961\n",
      "Training epoch: 647\n",
      "Loss: 0.16437994051045832\n",
      "Training epoch: 648\n",
      "Loss: 0.1643592028885171\n",
      "Training epoch: 649\n",
      "Loss: 0.1643384678890151\n",
      "Training epoch: 650\n",
      "Loss: 0.1643177355116215\n",
      "Training epoch: 651\n",
      "Loss: 0.1642970057560053\n",
      "Training epoch: 652\n",
      "Loss: 0.16427627862183597\n",
      "Training epoch: 653\n",
      "Loss: 0.1642555541087825\n",
      "Training epoch: 654\n",
      "Loss: 0.1642348322165144\n",
      "Training epoch: 655\n",
      "Loss: 0.16421411294470087\n",
      "Training epoch: 656\n",
      "Loss: 0.1641933962930113\n",
      "Training epoch: 657\n",
      "Loss: 0.16417268226111517\n",
      "Training epoch: 658\n",
      "Loss: 0.16415197084868197\n",
      "Training epoch: 659\n",
      "Loss: 0.16413126205538112\n",
      "Training epoch: 660\n",
      "Loss: 0.1641105558808822\n",
      "Training epoch: 661\n",
      "Loss: 0.16408985232485482\n",
      "Training epoch: 662\n",
      "Loss: 0.1640691513869686\n",
      "Training epoch: 663\n",
      "Loss: 0.16404845306689322\n",
      "Training epoch: 664\n",
      "Loss: 0.16402775736429837\n",
      "Training epoch: 665\n",
      "Loss: 0.16400706427885378\n",
      "Training epoch: 666\n",
      "Loss: 0.16398637381022943\n",
      "Training epoch: 667\n",
      "Loss: 0.16396568595809488\n",
      "Training epoch: 668\n",
      "Loss: 0.1639450007221202\n",
      "Training epoch: 669\n",
      "Loss: 0.16392431810197527\n",
      "Training epoch: 670\n",
      "Loss: 0.16390363809733005\n",
      "Training epoch: 671\n",
      "Loss: 0.16388296070785457\n",
      "Training epoch: 672\n",
      "Loss: 0.1638622859332188\n",
      "Training epoch: 673\n",
      "Loss: 0.16384161377309295\n",
      "Training epoch: 674\n",
      "Loss: 0.16382094422714705\n",
      "Training epoch: 675\n",
      "Loss: 0.1638002772950513\n",
      "Training epoch: 676\n",
      "Loss: 0.16377961297647595\n",
      "Training epoch: 677\n",
      "Loss: 0.1637589512710913\n",
      "Training epoch: 678\n",
      "Loss: 0.16373829217856753\n",
      "Training epoch: 679\n",
      "Loss: 0.16371763569857503\n",
      "Training epoch: 680\n",
      "Loss: 0.16369698183078418\n",
      "Training epoch: 681\n",
      "Loss: 0.16367633057486547\n",
      "Training epoch: 682\n",
      "Loss: 0.16365568193048924\n",
      "Training epoch: 683\n",
      "Loss: 0.1636350358973261\n",
      "Training epoch: 684\n",
      "Loss: 0.16361439247504656\n",
      "Training epoch: 685\n",
      "Loss: 0.1635937516633212\n",
      "Training epoch: 686\n",
      "Loss: 0.16357311346182069\n",
      "Training epoch: 687\n",
      "Loss: 0.16355247787021568\n",
      "Training epoch: 688\n",
      "Loss: 0.1635318448881769\n",
      "Training epoch: 689\n",
      "Loss: 0.16351121451537504\n",
      "Training epoch: 690\n",
      "Loss: 0.16349058675148104\n",
      "Training epoch: 691\n",
      "Loss: 0.16346996159616559\n",
      "Training epoch: 692\n",
      "Loss: 0.16344933904909964\n",
      "Training epoch: 693\n",
      "Loss: 0.16342871910995416\n",
      "Training epoch: 694\n",
      "Loss: 0.1634081017784\n",
      "Training epoch: 695\n",
      "Loss: 0.16338748705410827\n",
      "Training epoch: 696\n",
      "Loss: 0.16336687493675\n",
      "Training epoch: 697\n",
      "Loss: 0.16334626542599626\n",
      "Training epoch: 698\n",
      "Loss: 0.16332565852151817\n",
      "Training epoch: 699\n",
      "Loss: 0.1633050542229869\n",
      "Training epoch: 700\n",
      "Loss: 0.16328445253007368\n",
      "Training epoch: 701\n",
      "Loss: 0.16326385344244979\n",
      "Training epoch: 702\n",
      "Loss: 0.16324325695978648\n",
      "Training epoch: 703\n",
      "Loss: 0.16322266308175518\n",
      "Training epoch: 704\n",
      "Loss: 0.16320207180802718\n",
      "Training epoch: 705\n",
      "Loss: 0.1631814831382739\n",
      "Training epoch: 706\n",
      "Loss: 0.16316089707216686\n",
      "Training epoch: 707\n",
      "Loss: 0.16314031360937753\n",
      "Training epoch: 708\n",
      "Loss: 0.16311973274957747\n",
      "Training epoch: 709\n",
      "Loss: 0.16309915449243828\n",
      "Training epoch: 710\n",
      "Loss: 0.16307857883763155\n",
      "Training epoch: 711\n",
      "Loss: 0.16305800578482899\n",
      "Training epoch: 712\n",
      "Loss: 0.1630374353337023\n",
      "Training epoch: 713\n",
      "Loss: 0.1630168674839233\n",
      "Training epoch: 714\n",
      "Loss: 0.16299630223516365\n",
      "Training epoch: 715\n",
      "Loss: 0.1629757395870953\n",
      "Training epoch: 716\n",
      "Loss: 0.1629551795393901\n",
      "Training epoch: 717\n",
      "Loss: 0.1629346220917199\n",
      "Training epoch: 718\n",
      "Loss: 0.16291406724375676\n",
      "Training epoch: 719\n",
      "Loss: 0.16289351499517266\n",
      "Training epoch: 720\n",
      "Loss: 0.16287296534563966\n",
      "Training epoch: 721\n",
      "Loss: 0.16285241829482983\n",
      "Training epoch: 722\n",
      "Loss: 0.16283187384241526\n",
      "Training epoch: 723\n",
      "Loss: 0.16281133198806813\n",
      "Training epoch: 724\n",
      "Loss: 0.16279079273146066\n",
      "Training epoch: 725\n",
      "Loss: 0.16277025607226517\n",
      "Training epoch: 726\n",
      "Loss: 0.16274972201015386\n",
      "Training epoch: 727\n",
      "Loss: 0.16272919054479915\n",
      "Training epoch: 728\n",
      "Loss: 0.16270866167587333\n",
      "Training epoch: 729\n",
      "Loss: 0.16268813540304888\n",
      "Training epoch: 730\n",
      "Loss: 0.16266761172599825\n",
      "Training epoch: 731\n",
      "Loss: 0.1626470906443939\n",
      "Training epoch: 732\n",
      "Loss: 0.1626265721579084\n",
      "Training epoch: 733\n",
      "Loss: 0.16260605626621435\n",
      "Training epoch: 734\n",
      "Loss: 0.1625855429689844\n",
      "Training epoch: 735\n",
      "Loss: 0.1625650322658911\n",
      "Training epoch: 736\n",
      "Loss: 0.16254452415660733\n",
      "Training epoch: 737\n",
      "Loss: 0.16252401864080568\n",
      "Training epoch: 738\n",
      "Loss: 0.16250351571815905\n",
      "Training epoch: 739\n",
      "Loss: 0.16248301538834023\n",
      "Training epoch: 740\n",
      "Loss: 0.16246251765102207\n",
      "Training epoch: 741\n",
      "Loss: 0.16244202250587755\n",
      "Training epoch: 742\n",
      "Loss: 0.16242152995257955\n",
      "Training epoch: 743\n",
      "Loss: 0.1624010399908011\n",
      "Training epoch: 744\n",
      "Loss: 0.16238055262021528\n",
      "Training epoch: 745\n",
      "Loss: 0.16236006784049517\n",
      "Training epoch: 746\n",
      "Loss: 0.16233958565131376\n",
      "Training epoch: 747\n",
      "Loss: 0.16231910605234434\n",
      "Training epoch: 748\n",
      "Loss: 0.16229862904326015\n",
      "Training epoch: 749\n",
      "Loss: 0.16227815462373432\n",
      "Training epoch: 750\n",
      "Loss: 0.1622576827934402\n",
      "Training epoch: 751\n",
      "Loss: 0.16223721355205112\n",
      "Training epoch: 752\n",
      "Loss: 0.16221674689924037\n",
      "Training epoch: 753\n",
      "Loss: 0.1621962828346815\n",
      "Training epoch: 754\n",
      "Loss: 0.1621758213580479\n",
      "Training epoch: 755\n",
      "Loss: 0.16215536246901296\n",
      "Training epoch: 756\n",
      "Loss: 0.16213490616725043\n",
      "Training epoch: 757\n",
      "Loss: 0.1621144524524337\n",
      "Training epoch: 758\n",
      "Loss: 0.1620940013242365\n",
      "Training epoch: 759\n",
      "Loss: 0.16207355278233238\n",
      "Training epoch: 760\n",
      "Loss: 0.16205310682639512\n",
      "Training epoch: 761\n",
      "Loss: 0.16203266345609849\n",
      "Training epoch: 762\n",
      "Loss: 0.16201222267111615\n",
      "Training epoch: 763\n",
      "Loss: 0.16199178447112206\n",
      "Training epoch: 764\n",
      "Loss: 0.16197134885579004\n",
      "Training epoch: 765\n",
      "Loss: 0.16195091582479396\n",
      "Training epoch: 766\n",
      "Loss: 0.16193048537780777\n",
      "Training epoch: 767\n",
      "Loss: 0.1619100575145055\n",
      "Training epoch: 768\n",
      "Loss: 0.16188963223456113\n",
      "Training epoch: 769\n",
      "Loss: 0.16186920953764874\n",
      "Training epoch: 770\n",
      "Loss: 0.16184878942344252\n",
      "Training epoch: 771\n",
      "Loss: 0.16182837189161656\n",
      "Training epoch: 772\n",
      "Loss: 0.16180795694184508\n",
      "Training epoch: 773\n",
      "Loss: 0.16178754457380226\n",
      "Training epoch: 774\n",
      "Loss: 0.1617671347871624\n",
      "Training epoch: 775\n",
      "Loss: 0.16174672758159986\n",
      "Training epoch: 776\n",
      "Loss: 0.161726322956789\n",
      "Training epoch: 777\n",
      "Loss: 0.16170592091240413\n",
      "Training epoch: 778\n",
      "Loss: 0.1616855214481198\n",
      "Training epoch: 779\n",
      "Loss: 0.16166512456361043\n",
      "Training epoch: 780\n",
      "Loss: 0.1616447302585506\n",
      "Training epoch: 781\n",
      "Loss: 0.16162433853261482\n",
      "Training epoch: 782\n",
      "Loss: 0.1616039493854777\n",
      "Training epoch: 783\n",
      "Loss: 0.16158356281681388\n",
      "Training epoch: 784\n",
      "Loss: 0.16156317882629806\n",
      "Training epoch: 785\n",
      "Loss: 0.16154279741360508\n",
      "Training epoch: 786\n",
      "Loss: 0.16152241857840954\n",
      "Training epoch: 787\n",
      "Loss: 0.16150204232038637\n",
      "Training epoch: 788\n",
      "Loss: 0.16148166863921035\n",
      "Training epoch: 789\n",
      "Loss: 0.16146129753455638\n",
      "Training epoch: 790\n",
      "Loss: 0.16144092900609947\n",
      "Training epoch: 791\n",
      "Loss: 0.16142056305351454\n",
      "Training epoch: 792\n",
      "Loss: 0.16140019967647656\n",
      "Training epoch: 793\n",
      "Loss: 0.1613798388746607\n",
      "Training epoch: 794\n",
      "Loss: 0.161359480647742\n",
      "Training epoch: 795\n",
      "Loss: 0.1613391249953956\n",
      "Training epoch: 796\n",
      "Loss: 0.16131877191729665\n",
      "Training epoch: 797\n",
      "Loss: 0.16129842141312045\n",
      "Training epoch: 798\n",
      "Loss: 0.1612780734825422\n",
      "Training epoch: 799\n",
      "Loss: 0.16125772812523725\n",
      "Training epoch: 800\n",
      "Loss: 0.16123738534088095\n",
      "Training epoch: 801\n",
      "Loss: 0.16121704512914864\n",
      "Training epoch: 802\n",
      "Loss: 0.16119670748971576\n",
      "Training epoch: 803\n",
      "Loss: 0.1611763724222578\n",
      "Training epoch: 804\n",
      "Loss: 0.16115603992645025\n",
      "Training epoch: 805\n",
      "Loss: 0.16113571000196875\n",
      "Training epoch: 806\n",
      "Loss: 0.16111538264848874\n",
      "Training epoch: 807\n",
      "Loss: 0.16109505786568593\n",
      "Training epoch: 808\n",
      "Loss: 0.16107473565323605\n",
      "Training epoch: 809\n",
      "Loss: 0.16105441601081466\n",
      "Training epoch: 810\n",
      "Loss: 0.16103409893809773\n",
      "Training epoch: 811\n",
      "Loss: 0.16101378443476091\n",
      "Training epoch: 812\n",
      "Loss: 0.16099347250048004\n",
      "Training epoch: 813\n",
      "Loss: 0.16097316313493104\n",
      "Training epoch: 814\n",
      "Loss: 0.1609528563377899\n",
      "Training epoch: 815\n",
      "Loss: 0.16093255210873245\n",
      "Training epoch: 816\n",
      "Loss: 0.16091225044743473\n",
      "Training epoch: 817\n",
      "Loss: 0.16089195135357287\n",
      "Training epoch: 818\n",
      "Loss: 0.16087165482682284\n",
      "Training epoch: 819\n",
      "Loss: 0.16085136086686083\n",
      "Training epoch: 820\n",
      "Loss: 0.16083106947336295\n",
      "Training epoch: 821\n",
      "Loss: 0.1608107806460055\n",
      "Training epoch: 822\n",
      "Loss: 0.1607904943844647\n",
      "Training epoch: 823\n",
      "Loss: 0.16077021068841676\n",
      "Training epoch: 824\n",
      "Loss: 0.1607499295575381\n",
      "Training epoch: 825\n",
      "Loss: 0.16072965099150505\n",
      "Training epoch: 826\n",
      "Loss: 0.16070937498999405\n",
      "Training epoch: 827\n",
      "Loss: 0.16068910155268148\n",
      "Training epoch: 828\n",
      "Loss: 0.16066883067924395\n",
      "Training epoch: 829\n",
      "Loss: 0.16064856236935784\n",
      "Training epoch: 830\n",
      "Loss: 0.16062829662269987\n",
      "Training epoch: 831\n",
      "Loss: 0.1606080334389466\n",
      "Training epoch: 832\n",
      "Loss: 0.16058777281777478\n",
      "Training epoch: 833\n",
      "Loss: 0.16056751475886094\n",
      "Training epoch: 834\n",
      "Loss: 0.1605472592618819\n",
      "Training epoch: 835\n",
      "Loss: 0.16052700632651448\n",
      "Training epoch: 836\n",
      "Loss: 0.1605067559524354\n",
      "Training epoch: 837\n",
      "Loss: 0.16048650813932158\n",
      "Training epoch: 838\n",
      "Loss: 0.16046626288684995\n",
      "Training epoch: 839\n",
      "Loss: 0.16044602019469748\n",
      "Training epoch: 840\n",
      "Loss: 0.16042578006254107\n",
      "Training epoch: 841\n",
      "Loss: 0.16040554249005776\n",
      "Training epoch: 842\n",
      "Loss: 0.16038530747692467\n",
      "Training epoch: 843\n",
      "Loss: 0.1603650750228189\n",
      "Training epoch: 844\n",
      "Loss: 0.1603448451274175\n",
      "Training epoch: 845\n",
      "Loss: 0.16032461779039775\n",
      "Training epoch: 846\n",
      "Loss: 0.1603043930114369\n",
      "Training epoch: 847\n",
      "Loss: 0.16028417079021215\n",
      "Training epoch: 848\n",
      "Loss: 0.16026395112640085\n",
      "Training epoch: 849\n",
      "Loss: 0.16024373401968034\n",
      "Training epoch: 850\n",
      "Loss: 0.16022351946972804\n",
      "Training epoch: 851\n",
      "Loss: 0.1602033074762213\n",
      "Training epoch: 852\n",
      "Loss: 0.1601830980388377\n",
      "Training epoch: 853\n",
      "Loss: 0.16016289115725474\n",
      "Training epoch: 854\n",
      "Loss: 0.16014268683114985\n",
      "Training epoch: 855\n",
      "Loss: 0.1601224850602008\n",
      "Training epoch: 856\n",
      "Loss: 0.16010228584408515\n",
      "Training epoch: 857\n",
      "Loss: 0.1600820891824806\n",
      "Training epoch: 858\n",
      "Loss: 0.16006189507506474\n",
      "Training epoch: 859\n",
      "Loss: 0.16004170352151553\n",
      "Training epoch: 860\n",
      "Loss: 0.16002151452151062\n",
      "Training epoch: 861\n",
      "Loss: 0.16000132807472794\n",
      "Training epoch: 862\n",
      "Loss: 0.15998114418084536\n",
      "Training epoch: 863\n",
      "Loss: 0.15996096283954078\n",
      "Training epoch: 864\n",
      "Loss: 0.1599407840504921\n",
      "Training epoch: 865\n",
      "Loss: 0.15992060781337747\n",
      "Training epoch: 866\n",
      "Loss: 0.1599004341278748\n",
      "Training epoch: 867\n",
      "Loss: 0.1598802629936623\n",
      "Training epoch: 868\n",
      "Loss: 0.15986009441041799\n",
      "Training epoch: 869\n",
      "Loss: 0.15983992837782005\n",
      "Training epoch: 870\n",
      "Loss: 0.15981976489554672\n",
      "Training epoch: 871\n",
      "Loss: 0.1597996039632763\n",
      "Training epoch: 872\n",
      "Loss: 0.15977944558068694\n",
      "Training epoch: 873\n",
      "Loss: 0.15975928974745712\n",
      "Training epoch: 874\n",
      "Loss: 0.15973913646326512\n",
      "Training epoch: 875\n",
      "Loss: 0.1597189857277893\n",
      "Training epoch: 876\n",
      "Loss: 0.15969883754070827\n",
      "Training epoch: 877\n",
      "Loss: 0.1596786919017004\n",
      "Training epoch: 878\n",
      "Loss: 0.1596585488104443\n",
      "Training epoch: 879\n",
      "Loss: 0.15963840826661846\n",
      "Training epoch: 880\n",
      "Loss: 0.15961827026990155\n",
      "Training epoch: 881\n",
      "Loss: 0.15959813481997218\n",
      "Training epoch: 882\n",
      "Loss: 0.15957800191650912\n",
      "Training epoch: 883\n",
      "Loss: 0.1595578715591911\n",
      "Training epoch: 884\n",
      "Loss: 0.15953774374769678\n",
      "Training epoch: 885\n",
      "Loss: 0.15951761848170512\n",
      "Training epoch: 886\n",
      "Loss: 0.1594974957608949\n",
      "Training epoch: 887\n",
      "Loss: 0.15947737558494496\n",
      "Training epoch: 888\n",
      "Loss: 0.15945725795353444\n",
      "Training epoch: 889\n",
      "Loss: 0.15943714286634214\n",
      "Training epoch: 890\n",
      "Loss: 0.1594170303230471\n",
      "Training epoch: 891\n",
      "Loss: 0.15939692032332847\n",
      "Training epoch: 892\n",
      "Loss: 0.15937681286686523\n",
      "Training epoch: 893\n",
      "Loss: 0.15935670795333667\n",
      "Training epoch: 894\n",
      "Loss: 0.15933660558242185\n",
      "Training epoch: 895\n",
      "Loss: 0.15931650575380008\n",
      "Training epoch: 896\n",
      "Loss: 0.1592964084671505\n",
      "Training epoch: 897\n",
      "Loss: 0.15927631372215256\n",
      "Training epoch: 898\n",
      "Loss: 0.15925622151848556\n",
      "Training epoch: 899\n",
      "Loss: 0.15923613185582883\n",
      "Training epoch: 900\n",
      "Loss: 0.15921604473386183\n",
      "Training epoch: 901\n",
      "Loss: 0.15919596015226406\n",
      "Training epoch: 902\n",
      "Loss: 0.1591758781107149\n",
      "Training epoch: 903\n",
      "Loss: 0.15915579860889414\n",
      "Training epoch: 904\n",
      "Loss: 0.15913572164648113\n",
      "Training epoch: 905\n",
      "Loss: 0.1591156472231556\n",
      "Training epoch: 906\n",
      "Loss: 0.15909557533859725\n",
      "Training epoch: 907\n",
      "Loss: 0.15907550599248574\n",
      "Training epoch: 908\n",
      "Loss: 0.1590554391845008\n",
      "Training epoch: 909\n",
      "Loss: 0.1590353749143223\n",
      "Training epoch: 910\n",
      "Loss: 0.15901531318162998\n",
      "Training epoch: 911\n",
      "Loss: 0.15899525398610376\n",
      "Training epoch: 912\n",
      "Loss: 0.15897519732742357\n",
      "Training epoch: 913\n",
      "Loss: 0.15895514320526938\n",
      "Training epoch: 914\n",
      "Loss: 0.15893509161932107\n",
      "Training epoch: 915\n",
      "Loss: 0.15891504256925876\n",
      "Training epoch: 916\n",
      "Loss: 0.1588949960547625\n",
      "Training epoch: 917\n",
      "Loss: 0.1588749520755125\n",
      "Training epoch: 918\n",
      "Loss: 0.15885491063118876\n",
      "Training epoch: 919\n",
      "Loss: 0.15883487172147157\n",
      "Training epoch: 920\n",
      "Loss: 0.15881483534604113\n",
      "Training epoch: 921\n",
      "Loss: 0.15879480150457773\n",
      "Training epoch: 922\n",
      "Loss: 0.1587747701967617\n",
      "Training epoch: 923\n",
      "Loss: 0.1587547414222734\n",
      "Training epoch: 924\n",
      "Loss: 0.15873471518079318\n",
      "Training epoch: 925\n",
      "Loss: 0.15871469147200154\n",
      "Training epoch: 926\n",
      "Loss: 0.15869467029557893\n",
      "Training epoch: 927\n",
      "Loss: 0.1586746516512058\n",
      "Training epoch: 928\n",
      "Loss: 0.15865463553856285\n",
      "Training epoch: 929\n",
      "Loss: 0.15863462195733063\n",
      "Training epoch: 930\n",
      "Loss: 0.15861461090718965\n",
      "Training epoch: 931\n",
      "Loss: 0.1585946023878208\n",
      "Training epoch: 932\n",
      "Loss: 0.15857459639890464\n",
      "Training epoch: 933\n",
      "Loss: 0.15855459294012203\n",
      "Training epoch: 934\n",
      "Loss: 0.15853459201115375\n",
      "Training epoch: 935\n",
      "Loss: 0.15851459361168052\n",
      "Training epoch: 936\n",
      "Loss: 0.1584945977413834\n",
      "Training epoch: 937\n",
      "Loss: 0.15847460439994324\n",
      "Training epoch: 938\n",
      "Loss: 0.15845461358704097\n",
      "Training epoch: 939\n",
      "Loss: 0.15843462530235766\n",
      "Training epoch: 940\n",
      "Loss: 0.15841463954557436\n",
      "Training epoch: 941\n",
      "Loss: 0.15839465631637203\n",
      "Training epoch: 942\n",
      "Loss: 0.15837467561443191\n",
      "Training epoch: 943\n",
      "Loss: 0.15835469743943514\n",
      "Training epoch: 944\n",
      "Loss: 0.15833472179106292\n",
      "Training epoch: 945\n",
      "Loss: 0.15831474866899645\n",
      "Training epoch: 946\n",
      "Loss: 0.1582947780729171\n",
      "Training epoch: 947\n",
      "Loss: 0.15827481000250618\n",
      "Training epoch: 948\n",
      "Loss: 0.15825484445744495\n",
      "Training epoch: 949\n",
      "Loss: 0.15823488143741493\n",
      "Training epoch: 950\n",
      "Loss: 0.15821492094209758\n",
      "Training epoch: 951\n",
      "Loss: 0.15819496297117427\n",
      "Training epoch: 952\n",
      "Loss: 0.15817500752432667\n",
      "Training epoch: 953\n",
      "Loss: 0.15815505460123627\n",
      "Training epoch: 954\n",
      "Loss: 0.15813510420158464\n",
      "Training epoch: 955\n",
      "Loss: 0.15811515632505344\n",
      "Training epoch: 956\n",
      "Loss: 0.15809521097132445\n",
      "Training epoch: 957\n",
      "Loss: 0.15807526814007936\n",
      "Training epoch: 958\n",
      "Loss: 0.15805532783099988\n",
      "Training epoch: 959\n",
      "Loss: 0.1580353900437679\n",
      "Training epoch: 960\n",
      "Loss: 0.15801545477806517\n",
      "Training epoch: 961\n",
      "Loss: 0.15799552203357367\n",
      "Training epoch: 962\n",
      "Loss: 0.15797559180997528\n",
      "Training epoch: 963\n",
      "Loss: 0.157955664106952\n",
      "Training epoch: 964\n",
      "Loss: 0.15793573892418586\n",
      "Training epoch: 965\n",
      "Loss: 0.15791581626135887\n",
      "Training epoch: 966\n",
      "Loss: 0.15789589611815308\n",
      "Training epoch: 967\n",
      "Loss: 0.15787597849425067\n",
      "Training epoch: 968\n",
      "Loss: 0.15785606338933386\n",
      "Training epoch: 969\n",
      "Loss: 0.1578361508030848\n",
      "Training epoch: 970\n",
      "Loss: 0.15781624073518571\n",
      "Training epoch: 971\n",
      "Loss: 0.15779633318531902\n",
      "Training epoch: 972\n",
      "Loss: 0.15777642815316686\n",
      "Training epoch: 973\n",
      "Loss: 0.15775652563841175\n",
      "Training epoch: 974\n",
      "Loss: 0.1577366256407361\n",
      "Training epoch: 975\n",
      "Loss: 0.15771672815982232\n",
      "Training epoch: 976\n",
      "Loss: 0.15769683319535285\n",
      "Training epoch: 977\n",
      "Loss: 0.15767694074701039\n",
      "Training epoch: 978\n",
      "Loss: 0.15765705081447728\n",
      "Training epoch: 979\n",
      "Loss: 0.15763716339743633\n",
      "Training epoch: 980\n",
      "Loss: 0.1576172784955701\n",
      "Training epoch: 981\n",
      "Loss: 0.15759739610856127\n",
      "Training epoch: 982\n",
      "Loss: 0.15757751623609267\n",
      "Training epoch: 983\n",
      "Loss: 0.15755763887784696\n",
      "Training epoch: 984\n",
      "Loss: 0.157537764033507\n",
      "Training epoch: 985\n",
      "Loss: 0.15751789170275565\n",
      "Training epoch: 986\n",
      "Loss: 0.1574980218852758\n",
      "Training epoch: 987\n",
      "Loss: 0.15747815458075043\n",
      "Training epoch: 988\n",
      "Loss: 0.15745828978886245\n",
      "Training epoch: 989\n",
      "Loss: 0.15743842750929485\n",
      "Training epoch: 990\n",
      "Loss: 0.15741856774173082\n",
      "Training epoch: 991\n",
      "Loss: 0.15739871048585324\n",
      "Training epoch: 992\n",
      "Loss: 0.15737885574134547\n",
      "Training epoch: 993\n",
      "Loss: 0.1573590035078905\n",
      "Training epoch: 994\n",
      "Loss: 0.15733915378517171\n",
      "Training epoch: 995\n",
      "Loss: 0.1573193065728722\n",
      "Training epoch: 996\n",
      "Loss: 0.15729946187067534\n",
      "Training epoch: 997\n",
      "Loss: 0.15727961967826448\n",
      "Training epoch: 998\n",
      "Loss: 0.15725977999532295\n",
      "Training epoch: 999\n",
      "Loss: 0.15723994282153425\n",
      "Training epoch: 1000\n",
      "Loss: 0.1572201081565817\n",
      "[[1.15806782]\n",
      " [0.27663556]]\n"
     ]
    }
   ],
   "source": [
    "w = training(x,y_train, alpha = 0)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(x1, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1000,6) and (2,1) not aligned: 6 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9220/1501369020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'3d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1000,6) and (2,1) not aligned: 6 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "one = np.ones((x_train.shape[0],1))\n",
    "x_train = np.concatenate((x_train, one),axis = 1)\n",
    "predict = x_train.dot(w)\n",
    "print(predict.shape)\n",
    "bx = plt.axes(projection='3d')\n",
    "bx.plot3D(x1[:,0],x2[:,0], predict[:,0],'gray')\n",
    "bx.scatter(x1[:,0],x2[:,0], predict[:,0], c = predict[:,0], cmap = 'Greens')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b003d8754f68205f230b91d29752a0899194161977d5c87b83f80e078894fa96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
